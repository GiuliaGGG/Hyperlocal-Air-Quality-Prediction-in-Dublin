{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5940b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83455033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_test = pd.read_csv('prepped_data/test_data.csv')\n",
    "df_train = pd.read_csv('prepped_data/train_data.csv')\n",
    "\n",
    "#split the training data into training and validation\n",
    "df_val = df_train[df_train['date'] > '2022-03-01']\n",
    "df_train = df_train[df_train['date'] <= '2022-03-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26dddb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Split train into X and Y\n",
    "Xtrain = df_train.iloc[:, 8:].copy()\n",
    "ytrain = df_train[\"PM25_ugm3\"].copy()\n",
    "\n",
    "# #Split test into X and Y\n",
    "Xtest = df_test.iloc[:, 8:].copy()\n",
    "ytest = df_test[\"PM25_ugm3\"].copy()\n",
    "\n",
    "# #Split val into X and Y\n",
    "Xval = df_val.iloc[:, 8:].copy()\n",
    "yval = df_val[\"PM25_ugm3\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c286bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running a simple imputer and scaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "#imputer = SimpleImputer(strategy = \"mean\")\n",
    "imputer = SimpleImputer()\n",
    "Xtrain = imputer.fit_transform(Xtrain)\n",
    "Xtest = imputer.transform(Xtest)\n",
    "Xval = imputer.transform(Xval)\n",
    "\n",
    "# Convert arrays back to DataFrame for easier manipulation\n",
    "Xtrain = pd.DataFrame(Xtrain, columns=df_train.columns[8:])\n",
    "Xtest = pd.DataFrame(Xtest, columns=df_test.columns[8:])\n",
    "Xval = pd.DataFrame(Xval, columns=df_val.columns[8:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02528e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming you know the names of the categorical columns\n",
    "categorical_columns = ['SiteID', 'day_of_week']  # List of categorical column names\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit the encoder on the training data\n",
    "encoder.fit(Xtrain[categorical_columns])\n",
    "\n",
    "# Transform both training and test data\n",
    "Xtrain_encoded = encoder.transform(Xtrain[categorical_columns])\n",
    "Xtest_encoded = encoder.transform(Xtest[categorical_columns])\n",
    "Xval_encoded = encoder.transform(Xval[categorical_columns])\n",
    "\n",
    "# Convert the encoded data to a DataFrame\n",
    "Xtrain_encoded = pd.DataFrame(Xtrain_encoded.toarray(), columns=encoder.get_feature_names_out(categorical_columns))\n",
    "Xtest_encoded = pd.DataFrame(Xtest_encoded.toarray(), columns=encoder.get_feature_names_out(categorical_columns))\n",
    "Xval_encoded = pd.DataFrame(Xval_encoded.toarray(), columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "# Drop the original categorical columns from the training and test data\n",
    "Xtrain = Xtrain.drop(columns=categorical_columns)\n",
    "Xtest = Xtest.drop(columns=categorical_columns)\n",
    "Xval = Xval.drop(columns=categorical_columns)\n",
    "\n",
    "# Concatenate the one-hot encoded columns to the training and test data\n",
    "Xtrain = pd.concat([Xtrain, Xtrain_encoded], axis=1)\n",
    "Xtest = pd.concat([Xtest, Xtest_encoded], axis=1)\n",
    "Xval = pd.concat([Xval, Xval_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0c74e7",
   "metadata": {},
   "source": [
    "Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a62bce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "Xtrain_scaled = scaler.fit_transform(Xtrain)\n",
    "Xtest_scaled = scaler.transform(Xtest)\n",
    "Xval_scaled = scaler.transform(Xval)\n",
    "\n",
    "# Convert arrays back to DataFrame for easier manipulation\n",
    "Xtrain = pd.DataFrame(Xtrain_scaled, columns=Xtrain.columns)\n",
    "Xtest = pd.DataFrame(Xtest_scaled, columns=Xtest.columns)\n",
    "Xval = pd.DataFrame(Xval_scaled, columns=Xval.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92e23bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bs = 64\n",
    "val_bs = 64\n",
    "test_bs = 64\n",
    "#Batchsize is a Hyperparameter, it is a tradeoff between speed and accuracy. We use 32 here to start as this was the value from the tutorials.\n",
    "\n",
    "# Create tensor datasets\n",
    "train_dataset = TensorDataset(torch.tensor(Xtrain.values).float(), torch.tensor(ytrain.values).unsqueeze(1).float())\n",
    "val_dataset = TensorDataset(torch.tensor(Xval.values).float(), torch.tensor(yval.values).unsqueeze(1).float())\n",
    "test_dataset = TensorDataset(torch.tensor(Xtest.values).float(), torch.tensor(ytest.values).unsqueeze(1).float())\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=train_bs, shuffle=True, drop_last=True) #dataset\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_bs, shuffle=False, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=test_bs, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f0297",
   "metadata": {},
   "source": [
    "Start of Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bffcbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimiser):\n",
    "    avg_loss = 0 # to store running loss\n",
    "    model.train() # Set model in training mode\n",
    "    for batch_idx, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X) # make prediction on current batch\n",
    "        loss = loss_fn(pred,y) # calculate loss\n",
    "        loss.backward() # calculates gradients\n",
    "        optimiser.step() # update weights\n",
    "        optimiser.zero_grad() # set gradients to zero for next batch\n",
    "        avg_loss += loss.item() # add loss to running loss\n",
    "    print(f'Average training Loss: {avg_loss/len(dataloader):.5f}')\n",
    "    train_loss = avg_loss/len(dataloader)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e168a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop(dataloader, model, loss_fn):\n",
    "    loss = 0\n",
    "    model.eval()\n",
    "    bs = dataloader.batch_size\n",
    "    for batch_idx, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X) # make prediction on current batch\n",
    "        loss += loss_fn(pred,y).item() # calculate loss\n",
    "\n",
    "    loss /= len(dataloader)\n",
    "    print(f'Avg val loss: {loss:.5f} \\n')\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22094281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinearity_constructor(name):\n",
    "    if (name == \"relu\"):\n",
    "        return nn.ReLU()\n",
    "    if (name == \"sigmoid\"):\n",
    "        return nn.Sigmoid()\n",
    "    if (name == \"leakyrelu\"):\n",
    "        return nn.LeakyReLU()\n",
    "    raise ValueError(\"Unknown nonlinearity!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8feecf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Average training Loss: 60.27385\n",
      "Avg val loss: 25.63456 \n",
      "\n",
      "Epoch 2\n",
      "Average training Loss: 53.67719\n",
      "Avg val loss: 35.32888 \n",
      "\n",
      "Epoch 3\n",
      "Average training Loss: 52.87491\n",
      "Avg val loss: 26.45234 \n",
      "\n",
      "Epoch 4\n",
      "Average training Loss: 49.59874\n",
      "Avg val loss: 27.55585 \n",
      "\n",
      "Epoch 5\n",
      "Average training Loss: 47.64116\n",
      "Avg val loss: 59.69711 \n",
      "\n",
      "Epoch 6\n",
      "Average training Loss: 43.92708\n",
      "Avg val loss: 46.08252 \n",
      "\n",
      "Epoch 7\n",
      "Average training Loss: 43.40008\n",
      "Avg val loss: 48.38584 \n",
      "\n",
      "Epoch 8\n",
      "Average training Loss: 40.76896\n",
      "Avg val loss: 50.30310 \n",
      "\n",
      "Epoch 9\n",
      "Average training Loss: 38.60134\n",
      "Avg val loss: 54.62297 \n",
      "\n",
      "Epoch 10\n",
      "Average training Loss: 37.41964\n",
      "Avg val loss: 69.22416 \n",
      "\n",
      "Epoch 11\n",
      "Average training Loss: 32.51389\n",
      "Avg val loss: 281.64350 \n",
      "\n",
      "Epoch 12\n",
      "Average training Loss: 31.82360\n",
      "Avg val loss: 68.98284 \n",
      "\n",
      "Epoch 13\n",
      "Average training Loss: 35.83362\n",
      "Avg val loss: 133.47155 \n",
      "\n",
      "Epoch 14\n",
      "Average training Loss: 31.62163\n",
      "Avg val loss: 75.73818 \n",
      "\n",
      "Epoch 15\n",
      "Average training Loss: 29.85430\n",
      "Avg val loss: 82.41375 \n",
      "\n",
      "Epoch 16\n",
      "Average training Loss: 27.36699\n",
      "Avg val loss: 56.39695 \n",
      "\n",
      "Epoch 17\n",
      "Average training Loss: 28.48927\n",
      "Avg val loss: 102.55854 \n",
      "\n",
      "Epoch 18\n",
      "Average training Loss: 23.30995\n",
      "Avg val loss: 59.94889 \n",
      "\n",
      "Epoch 19\n",
      "Average training Loss: 23.10328\n",
      "Avg val loss: 90.47365 \n",
      "\n",
      "Epoch 20\n",
      "Average training Loss: 27.94237\n",
      "Avg val loss: 56.09857 \n",
      "\n",
      "Epoch 21\n",
      "Average training Loss: 26.02448\n",
      "Avg val loss: 63.73506 \n",
      "\n",
      "Epoch 22\n",
      "Average training Loss: 23.00772\n",
      "Avg val loss: 63.46382 \n",
      "\n",
      "Epoch 23\n",
      "Average training Loss: 22.07320\n",
      "Avg val loss: 74.94128 \n",
      "\n",
      "Epoch 24\n",
      "Average training Loss: 3424.75000\n",
      "Avg val loss: 97.61204 \n",
      "\n",
      "Epoch 25\n",
      "Average training Loss: 36.30064\n",
      "Avg val loss: 139.19803 \n",
      "\n",
      "Epoch 26\n",
      "Average training Loss: 25.42974\n",
      "Avg val loss: 57.04683 \n",
      "\n",
      "Epoch 27\n",
      "Average training Loss: 23.79786\n",
      "Avg val loss: 100.33668 \n",
      "\n",
      "Epoch 28\n",
      "Average training Loss: 21.29976\n",
      "Avg val loss: 98.66117 \n",
      "\n",
      "Epoch 29\n",
      "Average training Loss: 19.75674\n",
      "Avg val loss: 86.98628 \n",
      "\n",
      "Epoch 30\n",
      "Average training Loss: 18.08489\n",
      "Avg val loss: 81.57415 \n",
      "\n",
      "Epoch 31\n",
      "Average training Loss: 18.04369\n",
      "Avg val loss: 105.27674 \n",
      "\n",
      "Epoch 32\n",
      "Average training Loss: 17.49249\n",
      "Avg val loss: 97.58659 \n",
      "\n",
      "Epoch 33\n",
      "Average training Loss: 20.64660\n",
      "Avg val loss: 63.43700 \n",
      "\n",
      "Epoch 34\n",
      "Average training Loss: 25.26684\n",
      "Avg val loss: 53.19887 \n",
      "\n",
      "Epoch 35\n",
      "Average training Loss: 17.58546\n",
      "Avg val loss: 73.09896 \n",
      "\n",
      "Epoch 36\n",
      "Average training Loss: 17.21155\n",
      "Avg val loss: 83.39772 \n",
      "\n",
      "Epoch 37\n",
      "Average training Loss: 19.68094\n",
      "Avg val loss: 105.88711 \n",
      "\n",
      "Epoch 38\n",
      "Average training Loss: 16.74684\n",
      "Avg val loss: 72.00527 \n",
      "\n",
      "Epoch 39\n",
      "Average training Loss: 15.28335\n",
      "Avg val loss: 88.72026 \n",
      "\n",
      "Epoch 40\n",
      "Average training Loss: 15.16748\n",
      "Avg val loss: 81.67156 \n",
      "\n",
      "Epoch 41\n",
      "Average training Loss: 15.39586\n",
      "Avg val loss: 66.56061 \n",
      "\n",
      "Epoch 42\n",
      "Average training Loss: 15.74830\n",
      "Avg val loss: 99.93910 \n",
      "\n",
      "Epoch 43\n",
      "Average training Loss: 17.30675\n",
      "Avg val loss: 51.72884 \n",
      "\n",
      "Epoch 44\n",
      "Average training Loss: 17.03886\n",
      "Avg val loss: 90.90116 \n",
      "\n",
      "Epoch 45\n",
      "Average training Loss: 14.70158\n",
      "Avg val loss: 89.61697 \n",
      "\n",
      "Epoch 46\n",
      "Average training Loss: 15.90418\n",
      "Avg val loss: 51.95737 \n",
      "\n",
      "Epoch 47\n",
      "Average training Loss: 15.80475\n",
      "Avg val loss: 82.14109 \n",
      "\n",
      "Epoch 48\n",
      "Average training Loss: 17.50758\n",
      "Avg val loss: 41.56732 \n",
      "\n",
      "Epoch 49\n",
      "Average training Loss: 17.06096\n",
      "Avg val loss: 100.36896 \n",
      "\n",
      "Epoch 50\n",
      "Average training Loss: 15.44585\n",
      "Avg val loss: 42.35326 \n",
      "\n",
      "Epoch 51\n",
      "Average training Loss: 25.43654\n",
      "Avg val loss: 53.11467 \n",
      "\n",
      "Epoch 52\n",
      "Average training Loss: 15.59319\n",
      "Avg val loss: 73.02917 \n",
      "\n",
      "Epoch 53\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m     train_loss_nn1\u001b[38;5;241m.\u001b[39mappend(train_loop(train_dataloader, model, loss_fn, optimiser))\n\u001b[0;32m     33\u001b[0m     val_loss_nn1\u001b[38;5;241m.\u001b[39mappend(val_loop(val_dataloader, model, loss_fn))\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#plot the training and validation loss\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimiser)\u001b[0m\n\u001b[0;32m      2\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# to store running loss\u001b[39;00m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# Set model in training mode\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m      5\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(X) \u001b[38;5;66;03m# make prediction on current batch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(pred,y) \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n",
      "File \u001b[1;32mi:\\Anaconda\\envs\\dsa2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mi:\\Anaconda\\envs\\dsa2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mi:\\Anaconda\\envs\\dsa2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32mi:\\Anaconda\\envs\\dsa2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[1;32mi:\\Anaconda\\envs\\dsa2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mi:\\Anaconda\\envs\\dsa2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mi:\\Anaconda\\envs\\dsa2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mi:\\Anaconda\\envs\\dsa2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:213\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    211\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    212\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,hid_layer=[100,100,100,100,100],nonlinearity = 'relu'): #Start with a default of 10, can change this in the call\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hid_layer[0]),\n",
    "            nonlinearity_constructor(nonlinearity),\n",
    "            nn.Linear(hid_layer[0], hid_layer[1]),\n",
    "            nonlinearity_constructor(nonlinearity),\n",
    "            nn.Linear(hid_layer[1], hid_layer[2]),\n",
    "            nonlinearity_constructor(nonlinearity),\n",
    "            nn.Linear(hid_layer[2], hid_layer[3]),\n",
    "            nonlinearity_constructor(nonlinearity),\n",
    "            nn.Linear(hid_layer[3], hid_layer[4]),\n",
    "            nonlinearity_constructor(nonlinearity),\n",
    "            nn.Linear(hid_layer[4], 1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "input_dim = Xtrain.shape[1]\n",
    "model = Network(nonlinearity='leakyrelu')\n",
    "\n",
    "learning_rate = 0.005\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "train_loss_nn1 = []\n",
    "val_loss_nn1 = []\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    train_loss_nn1.append(train_loop(train_dataloader, model, loss_fn, optimiser))\n",
    "    val_loss_nn1.append(val_loop(val_dataloader, model, loss_fn))\n",
    "\n",
    "\n",
    "#plot the training and validation loss\n",
    "plt.plot(train_loss_nn1)\n",
    "plt.plot(val_loss_nn1)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cfec61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "784e2e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing a time series split on the training data to split it into 5 folds for cross validation\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "# Set the number of splits\n",
    "n_splits = 5  # You can adjust this as needed\n",
    "\n",
    "# Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "#make a \n",
    "def Cross_validation(model, Xtrain, Ytrain, tscv):\n",
    "    train_loss_nn1 = []\n",
    "    val_loss_nn1 = []\n",
    "    epochs = 40\n",
    "    for train_index, val_index in tscv.split(Xtrain):\n",
    "        X_train_fold, X_val_fold = Xtrain[train_index], Xtrain[val_index]\n",
    "        y_train_fold, y_val_fold = Ytrain[train_index], Ytrain[val_index]\n",
    "\n",
    "        train_dataset = TensorDataset(torch.tensor(X_train_fold).float(), torch.tensor(y_train_fold).float())\n",
    "        val_dataset= TensorDataset(torch.tensor(X_val_fold).float(), torch.tensor(y_val_fold).float())\n",
    "\n",
    "        ## Dataloader\n",
    "\n",
    "        train_bs = 32\n",
    "        val_bs = 32\n",
    "        #Batchsize is a Hyperparameter, it is a tradeoff between speed and accuracy. We use 32 here to start as this was the value from the tutorials.\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=train_bs, shuffle=True, drop_last=True) #dataset\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=val_bs, shuffle=False, drop_last=True)\n",
    "\n",
    "\n",
    "        # Now you have X_train_fold, y_train_fold as your training data for this fold\n",
    "        # And X_val_fold, y_val_fold as your validation data for this fold\n",
    "\n",
    "        # You can train your neural network model on X_train_fold, y_train_fold\n",
    "        # And evaluate it on X_val_fold, y_val_fold\n",
    "        # Repeat this process for each fold\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Epoch {epoch+1}')\n",
    "            train_loss_nn1.append(train_loop(train_dataloader, model, loss_fn, optimiser))\n",
    "            val_loss_nn1.append(val_test_loop(val_dataloader, model, loss_fn))\n",
    "    return train_loss_nn1, val_loss_nn1\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "213325c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\\n       ...\\n       5812, 5813, 5814, 5815, 5816, 5817, 5818, 5819, 5820, 5821],\\n      dtype='int32', length=5822)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m optimiser \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#train the model\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m train_loss_nn1, val_loss_nn1 \u001b[38;5;241m=\u001b[39m Cross_validation(model, Xtrain, Ytrain, tscv)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#plot the training and validation loss\u001b[39;00m\n\u001b[0;32m     25\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_loss_nn1)\n",
      "Cell \u001b[1;32mIn[13], line 15\u001b[0m, in \u001b[0;36mCross_validation\u001b[1;34m(model, Xtrain, Ytrain, tscv)\u001b[0m\n\u001b[0;32m     13\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m40\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, val_index \u001b[38;5;129;01min\u001b[39;00m tscv\u001b[38;5;241m.\u001b[39msplit(Xtrain):\n\u001b[1;32m---> 15\u001b[0m     X_train_fold, X_val_fold \u001b[38;5;241m=\u001b[39m Xtrain[train_index], Xtrain[val_index]\n\u001b[0;32m     16\u001b[0m     y_train_fold, y_val_fold \u001b[38;5;241m=\u001b[39m Ytrain[train_index], Ytrain[val_index]\n\u001b[0;32m     18\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(torch\u001b[38;5;241m.\u001b[39mtensor(X_train_fold)\u001b[38;5;241m.\u001b[39mfloat(), torch\u001b[38;5;241m.\u001b[39mtensor(y_train_fold)\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[1;32mi:\\Anaconda\\envs\\dsa2\\Lib\\site-packages\\pandas\\core\\frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mi:\\Anaconda\\envs\\dsa2\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mi:\\Anaconda\\envs\\dsa2\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\\n       ...\\n       5812, 5813, 5814, 5815, 5816, 5817, 5818, 5819, 5820, 5821],\\n      dtype='int32', length=5822)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self,hid_layer=[100]): #Start with a default of 10, can change this in the call\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hid_layer[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_layer[0], 1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "input_dim = Xtrain.shape[1]\n",
    "model = Network([10])\n",
    "\n",
    "learning_rate = 0.01\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#train the model\n",
    "train_loss_nn1, val_loss_nn1 = Cross_validation(model, Xtrain, Ytrain, tscv)\n",
    "\n",
    "#plot the training and validation loss\n",
    "plt.plot(train_loss_nn1)\n",
    "plt.plot(val_loss_nn1)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
